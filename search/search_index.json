{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reed Woyda's Electronic Lab notebook","text":""},{"location":"#usage","title":"Usage","text":"<p>The purpose of this site is to make available my on-going project notes</p> <p>This site was created using MKdock Template and jobindjohn/obsidian-pubpish-mkdocs</p>"},{"location":"Daily%20Logs/Daily%20Log%20Monday%20June%2012%202023/","title":"Daily Log Monday June 12 2023","text":""},{"location":"Daily%20Logs/Daily%20Log%20Monday%20June%2012%202023/#meetings","title":"Meetings:","text":"<p>10am with Kayla to discuss projects - Meeting with Kayla 06-12-2023 2pm with Kayla and Bridget - Personal Project Notes</p>"},{"location":"Daily%20Logs/Daily%20Log%20Monday%20June%2012%202023/#todo","title":"Todo:","text":"<ol> <li>Finish readings for Personal Project Notes</li> <li>Organize Obsidian vault for keeping track of everything I am doing in this job!</li> <li>Pull down the DRAM and DRAM2 grants and read them</li> <li>Get ready for Meeting with Rory 06-14-2023<ol> <li>Need to generate list of questions to ask him</li> <li>Need to read through all of the code for DRAM and DRAM2</li> </ol> </li> <li>Work on [[W2_2023_Omics_Workshop]], get through step 11 by Tuesday June 20th</li> </ol>"},{"location":"Daily%20Logs/Daily%20Log%20Monday%20June%2012%202023/#future-things","title":"Future things:","text":"<p>DRAM and DRAM2      group meeting was setup for 07/14/2023: DRAM2 Group Meeting July 14 2023 [[W2_2023_Omics_Workshop]]     For Tuesday June 20th, need to get through step 11 on: https://github.com/WrightonLabCSU/w2_multi-omics_crash_course/tree/main/02.%20How%20to%20process%20metagenomes-%20reads%20to%20MAGs</p>"},{"location":"Daily%20Logs/Daily%20Log%20Tuesday%20June%2013%202023/","title":"Daily Log Tuesday June 13 2023","text":""},{"location":"Daily%20Logs/Daily%20Log%20Tuesday%20June%2013%202023/#meetings","title":"Meetings:","text":"<p>No Meetings set for today</p>"},{"location":"Daily%20Logs/Daily%20Log%20Tuesday%20June%2013%202023/#todo","title":"Todo:","text":"<ul> <li>The main thing for today and tomorrow is to keep going through DRAM and DRAM2 and to build the [[List of Questions for Rory]] for the meeting: Meeting with Rory 06-14-2023<ul> <li>Read DRAM grants</li> </ul> </li> <li>Keep adding functionality to my Obsidian notebook<ul> <li>Maybe see how easily I can publish stuff on GitHub</li> <li>Want to add in Full Calendar and hook it up to Meeting notes and Daily Logs - DONE</li> </ul> </li> </ul>"},{"location":"Daily%20Logs/Daily%20Log%20Tuesday%20June%2013%202023/#future-things","title":"Future things:","text":""},{"location":"Meeting%20Notes/Meeting%20with%20Kayla%2006-12-2023/","title":"Meeting with Kayla 06-12-2023","text":"<p>Title: {{Meeting with Kayla}  Date: {{2023-06-12}}  Time: {{10:00}} </p> <p>Ongoing Projects/[[Grants]]:</p> <p>[[DRAM]]     OG DRAM - FUNDED         Never really had its own funding         Mike Schafer was the reason the pipeline became \"DRAM\"             The Wrighton lab had a pipeline but Mike wanted to build it into a real pipeline     DRAM-V         Matt Sullivan at OSU     DOE Comp Bio - Miller CU Denver - FUNDED         This is the grant that I am on and payed from         This grant is for improving annotations beyond homology         The idea is to incorporate what they are doing into DRAM/DRAM2         This also includes DRAM on KBASE: involves upkeep and such     DOE Comp Bio - submitted grant - Will find out in the Fall         AIMS             1. modularize DRAM                 1. divide into little \"Apps\"             2. New Features                 1. visualization/web dev. - this would involve hiring a web dev person             3. Integration with other platforms                 1. problem b/c databases are huge                 2. maybe create a DRAM \"light\"                     1. still to be used on a server                     2. This will need to go on the Microbiome CSU Group new server                         1. TUNE Grant from CSU                         2. DRAM \"light\" will go on this                     3. Modeling                         1. BioCrunch - LBNL                         2. EcoSys - Ecosystems scale model - can ingest microbial data                         3. Both 1. and 2. are relavent to EMERGE and Wetlands. We have money to get DRAM to make automated outputs and needs to be made friendly to make required output formats into 1. and 2.</p> <p>Other stuff related to DRAM     EMERGE/PermaFrost/Lakes         All permafrost grants         All have DRAM written into them in some way or form (mostly w.r.t. modeling)             i.e. genome function into models         EMERGE -OSU Matt Sullivan         Permafrost - DOD - NEW         Lakes - lakes that surround permafrost         Wetlands - Wrighton-led project</p> <p>DRAM and DRAM2     Kayla is sending latest (un-submitted) grant draft     This will be good to review to see what they want and what they said they would do     I need to read this</p> <p>Other projects - will prob. not work on     [[NIH - Salmonella]] - Ikia's stuff - money is gone - writing new grants         Maybe some short analyses for proof-of-concept stuff     NIH-Methylamines - grant being written</p> <p>In short...     All of these things depend on DRAM         have Kegg and other annotation databases     Some people are making other databases (i.e. related to shale fraking) which are packaged          These are really helpful for publishing and for data re-use and sharing     We want to have DRAM light and DRAM2 - which can bring in any pre-made databases \"modules\"</p> <p>Other Project and how they rely on DRAM     NIH-Methylamines - methyl compounds - annotations are poor so we have made our own databases and pipelines - NEEDS to be incorporated into DRAM         Has own pipeline and database (going to be \"finished\" on June 15th) - [[Dimitri]]             Probably will be not working             This pipeline also includes 3 gene sets which need to be called separately because they include stop codons which messes up the annotation             need to make a DRAM2 module: \"Pyrolysine-gene-caller\"</p> <p>Condensed Tanins - CAMPER - own databases and pipelines - now included in DRAM</p> <p>EMERGE Team - has own set of databases </p> <p>Meeting with [[Rory]] Date: {{2023-06-14}} Time: {{15:00}} : Meeting with Rory 06-14-2023     Modularization         Some are done but not all             NEED TO DETERMINE THIS     How to build \"packages\" i.e. for the Shale database and integrate them into DRAM     Passwords         ReadtheDocs         GitHub     Location of stuff on the server     Need to make list of questions for Rory: [[List of Questions for Rory]]</p> <p>Plan for DRAM:     Meet with Rory Date: {{2023-06-14}} Time: {{15:00}}: Meeting with Rory 06-14-2023     Work 3 days a week on DRAM (60%)     Work on other pipelines other days (40%)     Meet with DRAM team and outline what works and doesn't work: DRAM2 Group Meeting July 14 2023         Usually little things they have         This team also will test anything I need them to</p> <p>Wetlands Pipeline/Project     Timeline: this needs to be done by: August (but the priority is DRAM)     Goals         MAGs database         Write pipeline in Snakemake or Nextflow     Overview         Input: 120 metaG samples             reads             QC - megahit or idba             assembly - metabat2             bins - checkm/checkm2             QC - med/high filter</p> <p>These go into GROW database Want - to take new input reads and before going to the pipeline is to map these reads to the database, i.e. GROW or MUCC, and pull out these reads and jsut assemble what is left We can set a cutoff for how many of the reads map to the database (GROW or MUCC), for example 90%, and then not assemble these.</p> <p>Say you don't have a database, you will just assemble all of the reads into metagenomes and you get MAGS and this is now the new database.     Take these MAGS and map the raw reads to the generate MAGS         Then iteratively assembly until you run out of bins. Either bins are low or the reads left over are a small amount. </p> <p>3 options to run the pipeline as: 1. new data and just make a new MAG database     1. still choose assembler (IDBA or Megahit) 2. new data with iterative     1. run pipeline and iterative happens on the generated MAG database             1.needs to know when to stop the iteration (i.e. stop at 5 MAGs or 15Gbp) 3. New data with existing database     1. input new data and database     2. pulls out reads which map to the given database and then at the end, adds the MAGs to the new database     3. Option to add new MAGs to the given database or to not add them     4. For mapping you want to map reads that are semi-perfect i.e. 99%         1. But for a given Bin, they would need like 90% of reads mapped to it to be excluded</p> <p>At the end, you can take everything that didn't map and try to </p>"},{"location":"Meeting%20Notes/Meeting%20with%20Rory%2006-14-2023/","title":"Meeting with Roary 06-14-2023","text":""},{"location":"Meeting%20Notes/Meeting%20with%20Rory%2006-14-2023/#questions-for-roary","title":"Questions for Roary","text":"<ol> <li>Passwords<ol> <li>GitHub</li> <li>...</li> <li>...</li> <li>...</li> </ol> </li> <li>Where is stuff on the server<ol> <li>Walk me through the directories for both DRAM and DRAM2<ol> <li>..</li> </ol> </li> </ol> </li> <li>How did you typically work on DRAM and DRAM2<ol> <li>i.e. did you mainly work on the server to test stuff?<ol> <li>..</li> </ol> </li> <li>Do you have a version on your local machine to run and test stuff?<ol> <li>..</li> </ol> </li> </ol> </li> <li>Things you Love about DRAM<ol> <li>...</li> </ol> </li> <li>Things you Hate about DRAM<ol> <li>...</li> </ol> </li> <li>Things you Love about DRAM2<ol> <li>...</li> </ol> </li> <li>Things you hate about Dram2<ol> <li>...</li> </ol> </li> </ol>"},{"location":"Meeting%20Notes/Meeting%20with%20Rory%2006-14-2023/#stuff-i-want-to-go-over","title":"Stuff I want to go over","text":"<ol> <li>Walk me through how DRAM works from start to finish</li> <li>Same with DRAM2, walk me through DRAM2</li> <li></li> </ol>"},{"location":"Meeting%20Notes/Meeting%20with%20Rory%2006-14-2023/#current-state-of-dram2","title":"Current state of DRAM2","text":"<ul> <li>DRAM2 has individual commands (apps), some of which are the same from DRAM but some which are separated<ul> <li>i.e. DRAM: <code>DRAM.py annotate</code> in DRAM2: <code>dram2 .. call</code> and <code>dram2 annotate</code> </li> </ul> </li> <li>Which of these have been implemented and which have not been<ul> <li>Implemented<ul> <li>...</li> <li>...</li> </ul> </li> <li>Not done<ul> <li>...</li> <li>...</li> </ul> </li> </ul> </li> <li>These seem to be written in the GitHub DRAM2/dram2/<ul> <li>Which has:<ul> <li>annotate</li> <li>auto_protocol</li> <li>call_genes</li> <li>distill</li> <li>genbank</li> <li>strainer</li> <li>....</li> </ul> </li> <li>These each have their own <code>__init.py__</code> script along with (sometimes) some associated .py scripts</li> <li>It seems each of these are the \"modules\" and are invoked with the <code>dram2 ____</code><ul> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"Meeting%20Notes/Meeting%20with%20Rory%2006-14-2023/#snakemake","title":"Snakemake????","text":"<ol> <li>Show me the \"config\" file for Snakemake</li> <li>Explain how it runs and manages resources<ol> <li>Are the resources tunable by the user (Can show Nextflow config for reference)</li> </ol> </li> </ol>"},{"location":"People/Matt%20Sullivan/","title":"Matt Sullivan","text":"<p>OSU</p>"},{"location":"People/People/","title":"People","text":"<p>[[Rory]]</p> <p>[[Dimitri]]</p> <p>Kayla</p>"},{"location":"Projects/NIH-Methylamines/","title":"NIH Methylamines","text":""},{"location":"Projects/Projects/","title":"Projects","text":"<p>[[DRAM]]</p> <p>DRAM-V</p> <p>DRAM and DRAM2</p> <p>Wetlands</p> <p>[[NIH-Salmonella]]</p> <p>NIH-Methylamines</p>"},{"location":"Projects/Wetlands/","title":"Wetlands","text":""},{"location":"Projects/Wetlands/#goal","title":"Goal:","text":"<ol> <li>Goal of the pipeline is to take in raw reads and to generate MAGs</li> <li>Also, there will be 3 different options which the user can chose from to run the pipeline<ol> <li>These are outlined below</li> <li>In short, a user can just generate all the MAGs which can be generated from the reads. However, to speed up the process the user can specify a database to first map the reads to. These reads will then be discarded and the output will include which MAGs from the given database were identified. Then, the remaining reads will go through the pipeline to generate MAGs. Another option is for the user to chose to iterate on the MAGs. This is something I do not understand yet.</li> </ol> </li> </ol>"},{"location":"Projects/Wetlands/#pipeline-overview","title":"Pipeline Overview","text":"<p>Pipeline overview: (first went over in Meeting with Kayla 06-12-2023) 3 options to run the pipeline as: 1. new data and just make a new MAG database     1. still choose assembler (IDBA or Megahit) 2. new data with iterative     1. run pipeline and iterative happens on the generated MAG database             1.needs to know when to stop the iteration (i.e. stop at 5 MAGs or 15Gbp) 3. New data with existing database     1. input new data and database     2. pulls out reads which map to the given database and then at the end, adds the MAGs to the new database     3. Option to add new MAGs to the given database or to not add them     4. For mapping you want to map reads that are semi-perfect i.e. 99%         1. But for a given Bin, they would need like 90% of reads mapped to it to be excluded</p>"},{"location":"Projects/Wetlands/#unknownsquestions","title":"Unknowns/Questions","text":"<ol> <li>I am not sure yet what the \"iterative\" part is for option 2 of the pipeline.</li> <li>....lots of other stuff to keep adding</li> </ol>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/","title":"DRAM","text":"<ul> <li>Dram v1.4.4 still being used by some people</li> <li>However, most people are switching over to DRAM2 however, it does not have all of the functionality, yet.</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#implementation","title":"Implementation","text":"<ul> <li>Written in Python</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#databases","title":"Databases","text":""},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#-kegg-if-provided-by-the-user-uniref90-pfam-dbcan-refseq-viral-vogdb-and-the-merops-peptidase-database-as-well-as-custom-user-databases","title":"- KEGG\u00a0(if provided by the user),\u00a0UniRef90,\u00a0PFAM,\u00a0dbCAN,\u00a0RefSeq viral,\u00a0VOGDB\u00a0and the\u00a0MEROPS\u00a0peptidase database as well as custom user databases","text":""},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dependencies","title":"Dependencies","text":"<ul> <li>pandas,\u00a0networkx,\u00a0scikit-bio,\u00a0prodigal,\u00a0mmseqs2,\u00a0hmmer\u00a0and\u00a0tRNAscan-SE<ul> <li>This is if you do not use the Conda install</li> </ul> </li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#candidate-installation","title":"Candidate installation","text":"<ul> <li>This is if you want the latest version which is not typically pushed to Pypi or Bioconda <pre><code># Clone the git repository and move into it\ngit clone https://github.com/WrightonLabCSU/DRAM.git\ncd DRAM\n# Install dependencies, this will also install a stable version of DRAM that will then be replaced.\nconda env create --name my_dram_env -f environment.yaml\nconda activate my_dram_env\n# Install pip\nconda install pip3\npip3 install ./\n</code></pre></li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#documentation-and-information","title":"Documentation and Information","text":"<ul> <li>Details on DRAM and how DRAM works please see the\u00a0paper\u00a0as well as the\u00a0wiki.</li> <li>For information on how DRAM is changing, please read the most recent\u00a0release notes.</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#inputoutput","title":"Input/Output","text":""},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dram-annotate","title":"DRAM (annotate):","text":"<p>Purpose: Annotate MAGs; single or multiple</p> <p>Input: - fasta file or string with wildcards - Bin taxonomy     - TSV w/ 1st column being the bins and 2nd column with \u2018classification\u2019     - GTDB-tk output is in this format - Bin quality     - TSV w/ 1st column being the bins and 2nd column \u2018Completeness\u2019 and 3rd \u2018Contamination\u2019         - This is the output format of checkM - Output folder - Can also supple additional databases     - These can be HMMs or fastas and have required formats - Also need a CONFIG file</p> <p>Output: - Tab separated file (.tsv) with all the annotations from Pfam, KEGG, UniProt, dbCAN, and MEROPS databases for all genes in all the input genomes - GenBank files for each genome - Single gene-finding format (.gff) file of all annotations across genomes - Single fasta format file (.fasta) of each open reading frame nucleotide sequence and best ranked annotation (see Annotation grades section) - Single fasta format file (.fasta) of each translated open reading frame amino acid sequence and best ranked annotation KEGG annotation - Tab separated files (.tsv) with tRNAs and rRNAs</p>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dram-distill","title":"DRAM (distill):","text":"<p>Purpose: Summarize annotations (add more)</p> <p>Input: - annotations.tsv - output directory - rrnas.tsv (optional) - trnas.tsv (optional)</p> <p>Output: - genome_summary.xlsx     - summary of metabolisms present in each genome - genome_statistics.tsv     - contains all measures required by MIMAG (Minimum Information about a Metagenome-Assembled Genome) about each fasta used as input - liquor.html     - interactive HTML for gene pathways - **Can also provide custom Distill files for specific genes of interest</p>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dram-strainer","title":"DRAM (strainer):","text":"<p>Purpose: Further gene analysis by making trees of functional modeling</p> <p>Input: - You need to specify specific identifiers/genes or provide a TSV with specific genes of interest - annotations.tsv - input fasta - output location</p> <p>Output: - Depends on what you want to do. You can,     - Pull out a subset of genes to build a tree and get an output *.faa file     - Blast specific genes     - Pull out gene involved in a specific functional category         - i.e. bins from Roseburia genus that are involved in the TCA cycle             - --taxonomy Roseburia --categories TCA</p>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dram2","title":"DRAM2","text":""},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#implementation_1","title":"Implementation","text":"<ul> <li>Written in Python and SnakeMake<ul> <li>Used a config file for running<ul> <li>This is on the server for everyone <code>etc/dram_config.yaml</code> : dram_config.yaml</li> <li>However, a user can create their own config file which needs to be located in <code>~/.config/dram_config.yaml</code></li> </ul> </li> </ul> </li> </ul> <p>Being used for the Workshop and most people are using DRAM2 by the start of the Fall</p>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#dram2-vs-dram1-taken-from-dram2-readthedocs","title":"Dram2 vs Dram1 (taken from DRAM2 readthedocs)","text":"<ul> <li>It has a project config that can store run data. This is used for checking, and lets us skip most arguments.</li> <li>It has more steps than DRAM1, but each step is simpler.</li> <li>DRAM-v is not part of DRAM2. It may be in the future, but not currently.</li> <li>There is only one command, dram2 which lets you use all sub commands.</li> <li>Only DRAM2 has Adjectives and Phylogenetic Trees.</li> <li>Not everything from DRAM1 is implemented in DRAM2 yet.</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#how-dram2-is-run","title":"How DRAM2 is run","text":"<p><code>dram2 &lt;general_options&gt; &lt;sub_command&gt; &lt;sub_command_options&gt; &lt;arguments&gt;</code></p>"},{"location":"Projects/DRAM-Project/DRAM%20and%20DRAM2/#snakemake","title":"Snakemake","text":"<ul> <li>How does snakemake work? </li> </ul>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/","title":"DRAM2 DOE Grant","text":""},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#aims-overview","title":"Aims-Overview","text":"<ol> <li>improved DISTILLATE and PRODUCT level capabilities and visualizations</li> <li>integrating data from multiple biological modalities for refined annotation and profiling of community interactions</li> <li>expanding DRAM interoperability and training to enrich the user experience</li> </ol>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#key-deliverable","title":"Key Deliverable","text":"<p>The key deliverable of this proposed project is a major release of DRAM, here considered version 2 (DRAM2), in October 2024, with releases scheduled for the duration of this project.</p>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#detailed-aims","title":"Detailed Aims","text":""},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#aim1","title":"Aim1:","text":"<ul> <li>Modularize DRAM annotations and visualizations to enable annotation customization, efficiency, and scalability. A modular implementation of the DRAM algorithm provides users with customizable annotations for enhanced metabolic and ecosystem specificity and expert curation of traits with improved visualization. Outcomes from this aim will result in more precise annotations and updated visualizations, leading to readily available distillation of microbial traits and phenotypes from microbial genomic data.</li> <li>Deliverables for this aim include a revised trait app that incorporates carbon decomposition calls, along with new ecosystem and topic toolkits. Revised traits will not only enhance the user experience by enabling classification of MAGs into carbon use categories but will also be an integral part of predicting interactions in Aim 2 and model refinement in Aim 3. Additionally, our collaborations to build these toolkits will help us identify needed metabolisms and databases that should be incorporated into DRAM2. Lastly, it is our hope that standardizing the collection and reporting of ecosystem appropriate gene sets will facilitate more robust genomic analyses across laboratories and publications.</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#aim2","title":"Aim2:","text":"<ul> <li>Develop a multi-omics analysis toolkit for improved phenotypic and multi-organismal annotations. We will build pathway representatives of ecosystem critical annotations and improve statistical analyses through the ingestion of gene expression and metabolite data. Using our existing pathway completion feature, DRAM will calculate the most active pathways and improve inferences on pathway directionality, both of which are features needed to accurately infer metabolic handoffs and niche overlap.</li> <li>Deliverables from this aim include 2 new toolkits (ingest, multi-omics) and a new build trait app within the phenotype toolkit. New visualizations include (i) figures of pathways painted with multi-omic data, (ii) output of statistically enriched/depleted genes and pathways, and (iii) correlation plots and relevance network plots. We not only provide automation of steps manually done by our laboratories but provide users with the means to interactively explore their data and visualizations that provide new perspectives on the interactions between organisms.</li> </ul>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#aim3","title":"Aim3:","text":"<ul> <li>Enhance DRAM interoperability with existing, broadly used, genomics-based software and cultivate our user community. To make DRAM more interoperable with downstream applications we developed DRAM modes. These modes operate as pipelines which run customized analyses to generate automated outputs specific to their intended software targets. We will also offer training workshops to encourage user engagement, many of these in collaboration with the KBase team.</li> <li>Deliverables of Aim 3 include development of three modes of DRAM2 including DRAM2-genomemodel, DRAM2-traitmodel, and DRAM2-lite and their integration into four genome science platforms including KBase, BioCrunch, Aviary, and NMDC EDGE.\u00a0 To promote DRAM2 and strengthen usability, we will host 3 workshops/year for the project period. Beyond advancement of this genomics tool, this interoperability along with new features of DRAM2 (Aims 1 and 2) will advance genome annotation as a whole through rapid integration of new biochemistry into genome annotation (workshops), iterative validation between models and annotations (BioCrunch, KBase models), and standardization of metagenomic workflows (NMDC EDGE, Aviary). In summary, DRAM2 outlined here will become one of the core tools for systems biology research based on our capacity to consolidate functional annotations in terms of biochemical pathways and extend these findings to organismal interactions, biogeochemical cycles, and predictive frameworks.</li> </ul> <p>Figure from DOE grant: Schematic highlight of Aim 3 </p>"},{"location":"Projects/DRAM-Project/DRAM2-DOE-Grant/#figure-3-from-doe-grant","title":"Figure 3 from DOE grant","text":""},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/","title":"Continuous Workshop Notes","text":""},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#workshop-meeting-1-06082023","title":"Workshop Meeting #1 06/08/2023","text":""},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#things-to-change-for-future-workshops","title":"Things to change for future workshops:","text":"<pre><code>add the .sh file to run on the \"slurm\" slide i.e. sbatch &lt;script-name.sh&gt;\nsame with scancel i.e. scancel &lt;job id&gt;\nadd in visual, and more info, on the definition of sequencing coverage\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#todo","title":"TODO:","text":"<pre><code>1. Read Accurate Genomes from Metagenomes paper\n2. Read Dick chapters\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#workshop-meeting-2-06122023","title":"Workshop Meeting #2 06/12/2023","text":"<p>(not sure why this is not making a new sub-header...)</p>"},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#discussion-questions","title":"Discussion questions","text":"<pre><code>1. Chimeras \n2. Coverage - seems to be something people need a bit more help with\n3. Evolutionary-related stuff you can get from metagenomics - SNV/SNPs in a given MAG\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#3-approaches-to-metagenomics","title":"3 approaches to metagenomics","text":"<pre><code>1. Read based\n2. Gene based\n3. Genomic centric  What we do here.. mainly\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Continuous%20Workshop%20Notes/#todo_1","title":"TODO:","text":"<pre><code>1. Run through all of the steps in 02. reads to MAGs\n2. Record all steps and record and know all of the flags used what they mean/do\n3. Organize my directories \n4. Complete step 11 for next Monday\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Personal%20Project%20Notes/","title":"Personal Project Notes","text":"<ol> <li>Ran the copy and rename script and it was super quick!     This prompted me to make a SLURM folder in my email and auto direct the slurm outputs to this</li> <li>Next I ran the unzip_fastqc.sh script<ol> <li>This took longer, 29 minutes, mainly because the fastq files are 19G!</li> </ol> </li> </ol> NA # Reads Size (Gb) R1 b4 trim 54,616,206 19 R2 b4 trim 54,616,206 19 R1 after trim NA NA R2 after trim NA NA <p>R2 Quality across read length from FastQC </p> <p>R1 adapter content across read length from FastQC </p> <ol> <li>Next I ran the trimming using sickle_fastqc.sh<ol> <li>sickle pe -f ../raw_reads/WCRC_304_R1.fastq -r ../raw_reads/WCRC_304_R2.fastq -t sanger -o WCRC_304_R1_trimmed.fastq -p WCRC_304_R2_trimmed.fastq -s R1R2_singles.fastq</li> <li>We lost X reads from trimming</li> <li>Paste picture showing the quality and adapter content</li> </ol> </li> </ol> NA # Reads Size (Gb) R1 b4 trim 54,616,206 19 R2 b4 trim 54,616,206 19 R1 after trim 54,195,840 19 R2 after trim 54,195,840 19 <p></p> <p></p> <ol> <li>Next, re-zipped the reads using zip_raw.sh This frees up 38Gb<ol> <li>How many reads did we lose? 420,366 reads</li> <li>Does the number of R1 and R2 reads match? YES</li> <li>Have the quality metrics improved? YES</li> </ol> </li> <li> <p>Next, I used MEGAHIT to assemble the trimmed reads using the megahit.sh script: <pre><code>megahit -1 ../processed_reads/WCRC_304_R1_trimmed.fastq -2 ../processed_reads/WCRC_304_R2_trimmed.fastq --k-min 31 --k-max 121 --k-step 10 --mem-flag 1 -m 429496729600 -t 20\n</code></pre></p> </li> <li> <p>Next, assembly stats were generated using assembly_stats.sh insert table below</p> <ol> <li>How many contigs?</li> <li>N50 = ?</li> <li>Longest contig = ?</li> <li>Where else is this information found?</li> </ol> </li> <li>Next, Binning using MetaBat2 using the runMetaBat.sh script: map_bin.sh<ol> <li>First, scaffolds under 2499 bp were excluded using the pullseq.py script</li> <li>Then, mapped the trimmed reads to the scaffolds fasta and output a SAM file  using bbmap.sh</li> <li>Next, converted the SAM into a BAM and then sorted using samtools view and samtools sort</li> <li>Then, filtered for high quality mapping based on mapping score</li> <li>Lastly, used runMetaBat.sh to perform the binning</li> </ol> </li> <li> <p>Next, Bin QC and taxonomy using checkM</p> <ol> <li>Done using the checkm.sh</li> <li>How many HQ and MQ MAGs do you have (based on completion/contamination)?</li> <li>Are there MAGs with high strain heterogeneity? This measures the percentage of contaminating genes that come from the same lineage. What might this mean?</li> <li>Did your longest scaffolds bin?</li> <li>Check HQ/MQ MAGs: <pre><code>awk -F \"\\t\" '{if ($6 &gt;49 &amp;&amp; $7 &lt;11) print $1}' results.txt\n</code></pre></li> </ol> </li> <li> <p>Making a MAG database <pre><code># make MAG directory in your sample metaG folder\nmkdir MAGs\ncd MAGs\n# copy MQ + HQ MAGs to this directory\ncp ../assembly/megahit_out/WCRC_304_final.contigs_2500.fa.metabat-bins/bin.13.fa .\ncp ../assembly/megahit_out/WCRC_304_final.contigs_2500.fa.metabat-bins/bin.11.fa .\n# rename the MAGs\nmv bin.13.fa WCRC_304_bin.13.fa\nmv bin.11.fa WCRC_304_bin.11.fa\n</code></pre></p> </li> <li> <p>Evaluating MAG taxonomy uisng GTDB: gtdb.sh</p> <ol> <li>uses a set of 120 (bacterial) and 55 (archaeal) genes to construct a concatenated gene tree. Then, it uses a MAG's position in the tree relative to defined species to assign taxonomy. The GTDB website is a helpful resource to know for looking up the names.</li> </ol> </li> <li>Lasty, for this portion I did a cleanup by removing some stuff:</li> </ol> <pre><code>rm assembly/megahit_out/WCRC_304_final.contigs_2500_mapped.sam\nrm assembly/megahit_out/WCRC_304_final.contigs_2500_mapped.sorted.bam\nrm assembly/megahit_out/WCRC_304_final.contigs_2500_mapped99per.sorted.bam\ngzip assembly/megahit_out/WCRC_304_final.contigs_2500_mapped.bam\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/assembly_stats.sh/","title":"Assembly stats.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=20\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=400gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../assembly/megahit_out\n/ORG-Data/scripts/quicklooks/contig_stats.pl -i final.contigs.fa -o WCRC_304_final.contigs_STATS\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/checkm.sh/","title":"Checkm.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=20\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=200gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../assembly/megahit_out/WCRC_304_final.contigs_2500.fa.metabat-bins\ncheckm lineage_wf -t 20 -x fa . checkm\ncd checkm\ncheckm qa -o 2 -f results.txt --tab_table -t 20 lineage.ms .\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/copy_rename.sh/","title":"Copy rename.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=50gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../raw_reads \n# copy reads from ORG-DATA location \ncp /home/ORG-Data-2/Agribiome/cure_metaG/15_S10_L003_R* .\n# rename reads to sample name\nmv 15_S10_L003_R1_001.fastq.gz WCRC_304_R1.fastq.gz\nmv 15_S10_L003_R2_001.fastq.gz WCRC_304_R2.fastq.gz\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/gtdb.sh/","title":"Gtdb.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=20\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=200gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../MAGs\ngtdbtk classify_wf -x fa --genome_dir . --out_dir gtdb_v2.3.0_r214 --cpus 20 --mash_db /home/opt/gtdbtk/data/release214/mash/gtdb_ref_sketch.msh\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/map_bin.sh/","title":"Map bin.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=20\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=400gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../assembly/megahit_out\n# pull scaffolds &gt;= 2500 bp\npullseq.py -i final.contigs.fa -m 2500 -o WCRC_304_final.contigs_2500.fa\n# map to these scaffolds to get sam file\nbbmap.sh -Xmx48G threads=20 overwrite=t ref=WCRC_304_final.contigs_2500.fa in1=../../processed_reads/WCRC_304_R1_trimmed.fastq in2=../../processed_reads/WCRC_304_R2_trimmed.fastq out=WCRC_304_final.contigs_2500_mapped.sam\n# convert sam to bam, and sort\nsamtools view -@ 20 -bS WCRC_304_final.contigs_2500_mapped.sam &gt; WCRC_304_final.contigs_2500_mapped.bam\nsamtools sort -T WCRC_304_final.contigs_2500_mapped.sorted -o WCRC_304_final.contigs_2500_mapped.sorted.bam WCRC_304_final.contigs_2500_mapped.bam -@ 20\n# filter for high quality mapping (this step can be tunable and optional)\nreformat.sh -Xmx100g minidfilter=0.99 in=WCRC_304_final.contigs_2500_mapped.sorted.bam out=WCRC_304_final.contigs_2500_mapped99per.sorted.bam pairedonly=t primaryonly=t\n#bin\nrunMetaBat.sh WCRC_304_final.contigs_2500.fa WCRC_304_final.contigs_2500_mapped99per.sorted.bam\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/megahit.sh/","title":"Megahit.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=20\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=400gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../assembly\nmegahit -1 ../processed/WCRC_304_R1_trimmed.fastq -2 ../processed/WCRC_304_R2_trimmed.fastq --k-min 31 --k-max 121 --k-step 10 --mem-flag 1 -m 429496729600 -t 20\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/runMetaBat.sh/","title":"runMetaBat.sh","text":"<pre><code>#!/bin/bash\n\n\nSCRIPTPATH=\"$( cd \"$(dirname \"$0\")\" ; pwd -P )\"\nPATH=$SCRIPTPATH:$PATH\nMB=metabat2\nSUM=jgi_summarize_bam_contig_depths\nBADMAP=${BADMAP:=0}\n\n\n\nif ! $MB --help 2&gt;/dev/null\n\nthen\n\n\u00a0 echo \"Please ensure that the MetaBAT binaries are in your PATH: Could not find $MB\" 2&gt;&amp;1\n\n\u00a0 exit 1\n\nfi\n\n\n\nif ! $SUM 2&gt;/dev/null\n\nthen\n\n\u00a0 echo \"Please ensure that the MetaBAT binaries are in your PATH: Could not find $SUM\" 2&gt;&amp;1\n\n\u00a0 exit 1\n\nfi\n\n\n\nUSAGE=\"$0 &lt;select metabat options&gt; assembly.fa sample1.bam [ sample2.bam ...]\n\nYou can specify any metabat options EXCEPT:\n\n\u00a0 -i --inFile\n\n\u00a0 -o --outFile\n\n\u00a0 -a --abdFile\n\n\n\nFor full metabat options: $MB -h\n\n\"\n\n\n#outname=\n\n#metabatopts=\"--verbose --debug\"\n\nmetabatopts=\"\"\n\nfor arg in $@\n\ndo\n\n\u00a0 if [ -f \"$arg\" ]\n\n\u00a0 then\n\n\u00a0 \u00a0 break\n\n\u00a0 fi\n\n\u00a0 outname=\"${arg}\"\n\n\u00a0 metabatopts=\"$metabatopts $arg\"\n\n\u00a0 shift\n\ndone\n\n\nif [ $# -lt 2 ]\n\nthen\n\n\u00a0 echo \"$USAGE\" 1&gt;&amp;2\n\n\u00a0 exit 1\n\nfi\n\n\nassembly=$1\n\nshift\n\nif [ ! -f \"$assembly\" ]\n\nthen\n\n\u00a0 echo \"Please specify the assembly fasta file: $assembly does not exist\" 1&gt;&amp;2\n\n\u00a0 exit 1\n\nfi\n\nfor bam in $@\n\ndo\n\n\u00a0 if [ ! -f \"$bam\" ]\n\n\u00a0 then\n\n\u00a0 \u00a0 echo \"Could not find the expected bam file: $bam\" 1&gt;&amp;2\n\n\u00a0 \u00a0 exit 1\n\n\u00a0 fi\n\ndone\n\n\nset -e\n\n\ndepth=${assembly##*/}.depth.txt\n\npaired=${assembly##*/}.paired.txt\n\nlock=$depth.BUILDING\n\n\nwaitforlock()\n\n{\n\n\u00a0 while [ -L \"$lock\" ]\n\n\u00a0 do\n\n\u00a0 \u00a0 echo \"Waiting for $lock to be complete before continuing $(date)\"\n\n\u00a0 \u00a0 sleep 60\n\n\u00a0 done\n\n}\n\n\ncleanup()\n\n{\n\n\u00a0 rm -f $lock\n\n}\ntrap cleanup 0 1 2 3 15\n\n\n\nwaitforlock\n\nbadmap=${assembly##*/}.d\n\nbadmapopts=\n\nif [ \"$BADMAP\" != '0' ]\n\nthen\n\n\u00a0 mkdir -p ${badmap}\n\n\u00a0 badmapopts=\"--unmappedFastq ${badmap}/badmap\"\n\nfi\n\n\nif [ ! -f \"${depth}\" ] &amp;&amp; ln -s \"$(uname -n) $$\" $lock\n\nthen\n\n\u00a0 \u00a0 if [ ! -f \"${depth}\" ]\n\n\u00a0 \u00a0 then\n\n\u00a0 \u00a0 \u00a0 \u00a0 sumopts=\"--outputDepth ${depth} --pairedContigs ${paired} --minContigLength 1000 --minContigD$\n\n\u00a0 \u00a0 \u00a0 \u00a0 echo \"Executing: '$SUM $sumopts $@' at `date`\"\n\n\u00a0 \u00a0 \u00a0 \u00a0 $SUM $sumopts $@\n\n\u00a0 \u00a0 \u00a0 \u00a0 echo \"Finished $SUM at `date`\"\n\n\u00a0 \u00a0 fi\n\n\u00a0 \u00a0 echo \"Creating depth file for metabat at `date`\"\n\n\u00a0 \u00a0 rm $lock\n\nelse\n\n\u00a0 waitforlock\n\n\u00a0 echo \"Skipping $SUM as $depth already exists\"\n\nfi\n\noutname=${assembly##*/}.metabat-bins${outname}/bin\n\necho \"Executing: '$MB $metabatopts --inFile $assembly --outFile $outname --abdFile ${depth}' at `date$\n\n$MB $metabatopts --inFile $assembly --outFile $outname --abdFile ${depth}\n\necho \"Finished $MB at `date`\"\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/sickle_fastqc.sh/","title":"Sickle fastqc.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=10\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=100gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../processed_reads\nsickle pe -f ../raw_reads/WCRC_304_R1.fastq -r ../raw_reads/WCRC_304_R2.fastq -t sanger -o WCRC_304_R1_trimmed.fastq -p WCRC_304_R2_trimmed.fastq -s R1R2_singles.fastq\n# delete the singles file--we dont need it!\nrm R1R2_singles.fastq\nfastqc WCRC_304_R1_trimmed.fastq WCRC_304_R2_trimmed.fastq\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/unzip_fastqc.sh/","title":"Unzip fastqc.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=50gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../raw_reads\ngunzip WCRC_304*\nfastqc WCRC_304_R1.fastq WCRC_304_R2.fastq\n</code></pre>"},{"location":"W2%202023%20Omics%20Workshop/Scripts/zip_raw.sh/","title":"Zip raw.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --time=14-00:00:00\n#SBATCH --mem=50gb\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=reedrich@colostate.edu\n#SBATCH --partition=wrighton-hi,wrighton-low\n\ncd ../raw_reads\ngzip *fastq\n</code></pre>"}]}